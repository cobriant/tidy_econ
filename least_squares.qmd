# Least Squares

## Overview

In this chapter, we'll discuss what the method of least squares is and how it works.

In section [1.2](#least-squares-as-the-combination-of-observations), I'll introduce the method of least squares as the method to **combine observations** in order to make a guess about a linear relationship. In section [1.3](#deriving-ols-estimators-hatbeta_0-and-hatbeta_1), I'll derive OLS estimators from scratch by using the definition of the method of least squares. Finally in section [1.4](#numerical-example), I'll do a numerical example where you'll find $\hat{\beta_0}$ and $\hat{\beta_1}$ with 3 observations.

### Key Terms and Notation

| Symbol | Meaning | Example |
| ---: | ---------- | ----- |
| $\beta_0$ | Intercept parameter in a linear model | $y_i = \beta_0 + \beta_1 x_i + u_i$ |
| $\beta_1$ | Slope parameter in a linear model | see above |
| $y_i$ | dependent variable, outcome variable | see above |
| $x_i$ | explanatory variable | see above |
| $u_i$ | unobservable term, disturbance, shock | see above |
| $\hat{\beta}_0$ | Estimate of the intercept | $y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i$ |
| $\hat{\beta}_1$ | Estimate of the slope | see above |
| $\hat{y}_i$ | Fitted value, prediction | $\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i$ |
| $e_i$ | residual | $y_i - \hat{y}_i$ |

## Least Squares as the Combination of Observations

Suppose education (x) has a linear effect on wage (y). If someone has zero years of education, they will earn $5 per hour on average, and every extra year of education a person has results in an extra 50 cents added to their wage. Then a linear model would be the correct specification:

$$wage_i = \beta_0 + \beta_1 education_i + u_i$$

Where $\beta_0 = 5$ and $\beta_1 = 0.50$.

When we take some data on the education and earnings of a bunch of people, we could use OLS to *estimate* $\beta_0$ and $\beta_1$. I'll put hats on the betas to indicate they are estimates: $\hat{\beta_0}$ and $\hat{\beta_1}$ are our estimates of the true parameters $\beta_0$ and $\beta_1$. We might get $\hat{\beta_0} = 4$ and $\hat{\beta_1} = 0.75$ instead of the true values of the parameters $\beta_0 = 5$ and $\beta_1 = 0.50$.

$\beta_0$ is the true value of the intercept: if x takes on a 0, this is the expected value for y to take on. In mathematical terms, this is a conditional expectation: $E[y | x = 0] = \beta_0$, which is pronounced "the expectation of y **given** x takes 0 is $\beta_0$". And $\beta_1$ is the true effect of x on y: if x increases by one unit, $\beta_1$ is the amount by which y is expected to increase. In mathematical terms: $E[y | x = \alpha + 1] - E[y | x = \alpha] = \beta_1$ for any $\alpha$.

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ad205f8d-655c-4c79-b5ca-aec0010acd3c&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

[]: # (Question 1: Suppose the true model is $y = 2 + 5x$. What is E[y | x = 0]? What is E[y | x = 1]?)

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=53e23629-1052-44a3-b6df-aec0010c6e04&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

The method of least squares was first published by Frenchman Adrien Marie Legendre in 1805, but there is controversy about whether he was the first inventor or it was the German mathematician and physicist Carl Friedrich Gauss. The method of least squares founded the study of statistics, which was then called "the combination of observations," because that's what least squares helps you do: combine observations to understand a true underlying process. Least squares helped to solve two huge scientific problems in the beginning of the 1800s:

1. There's a field of science called Geodesy that was, at the time, concerned with measuring the circumference of the globe. They had measurements of distances between cities and angles of the stars at each of the cities, done by different observers through different procedures. But until least squares, they had no way to combine those observations.

2. Ceres (the largest object in the asteroid belt between Mars and Jupiter) was discovered. "Speculation about extra-terrestrial life on other planets was open to debate, and the potential new discovery of such a close neighbour to Earth was the buzz of the scientific community," @least_squares_web. Astronomers wanted to figure out the position and orbit of Ceres, but couldn't extrapolate that with only a few noisy observations. Until least squares came along.

The method of least squares quickly became the dominant way to solve this statistical problem and remains dominant today.

One reason the method of least squares is so popular is that it's so simple and mathematically tractable: the entire procedure can be summed up in one statement: **the method of least squares fits a linear model that minimizes the sum of the squared residuals.**

In the next few videos, we'll see that for a simple regression, we can take that statement of the method of least squares and derive:

$$\hat{\beta_0} = \bar{y} - \beta_1 \bar{x}$$

$$\hat{\beta_1} = \frac{\sum_i x_i y_i - \bar{x}\bar{y}n}{\sum_i x_i^2 - \bar{x}^2 n}$$

## Deriving OLS Estimators $\hat{\beta_0}$ and $\hat{\beta_1}$

> 3: Residuals are vertical distances: $e_i = y_i - \hat{y_i}$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=f0687af3-d1f8-4fb6-8041-aef20140dbce&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 4: OLS as $\displaystyle\min_{\hat{\beta_0}, \hat{\beta_1}} \sum_i e_i^2 = \min_{\hat{\beta_0}, \hat{\beta_1}} \sum_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=82872f65-d5d9-4c6b-aa22-aec0010d5697&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 5: $e_i^2 = y_i^2 - 2 \hat{\beta_0} y_i - 2 \hat{\beta_1} x_i y_i + 2 \hat{\beta_0} \hat{\beta_1} x_i + \hat{\beta_0}^2 + \hat{\beta_1}^2 x_i^2$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=03d5f4f3-ab42-4103-8b1f-aec0010de903&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 6: Some summation rules

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=710208aa-b2e3-42ad-8a5a-aec0010e7ac2&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

[Reference these summation rules in the future here.](math.html#summation-rules)

> 7: Taking first order conditions

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=6c304212-039b-48eb-9b07-aef2014ad04d&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 8: Simplifying the FOC for $\hat{\beta_0}$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ca241c5f-8fe2-47a1-8036-aec0010f30c3&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 9: Simplifying the FOC for $\hat{\beta_1}$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=d6b38fe0-4822-4e81-8aab-aef2014e2795&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>


## Numerical Example

> 10: Calculate $\hat{\beta_0}$ and $\hat{\beta_1}$ for a 3 observation example

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=de0d9eaa-2cd4-440c-b6c0-aef20150d34f&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 11: Calculate fitted values $\hat{y_i}$ and residuals $e_i$ for a 3 observation example

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=856e9bce-d8ce-4468-a8ef-aec001131222&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

> 12: $u_i$ versus $e_i$

<iframe src="https://uoregon.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=4019207b-cec5-44a9-bcf7-aec00114773f&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

## Exercises

[Classwork 1: Deriving OLS Estimators](classwork.html#cw1-deriving-ols-estimators-analytical)

[Koans 1-3: Vectors, Tibbles, and Pipes](/#install-the-tidyverse-koans)

[Classwork 2: lm and qplot](classwork.html#cw2-lm-and-qplot-r)

Koans 4-7: dplyr

[Classwork 3: dplyr murder mystery](classwork.html#cw3-dplyr-murder-mystery-r)

## References

@dougherty2016 Chapter 1: Simple Regression Analysis

@least_squares_web

